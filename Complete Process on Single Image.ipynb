{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from os import path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imutils\n",
    "from pprint import pprint\n",
    "\n",
    "IMG_DIR = Path(\"SingleImageProcess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_jpg(img, save_to):\n",
    "#     RGB = \"RGB\"\n",
    "    BW = \"L\"\n",
    "    old = str(img) #img is a path\n",
    "    old_name = str(img.name)\n",
    "    new_name = old_name.rstrip(str(img.suffix))\n",
    "    new_img = Image.open(img).convert(BW)\n",
    "    name = path.join(save_to, new_name+\".jpg\")\n",
    "    new_img.save(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. setup and convert images to jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wandalcooper/Library/Python/3.7/lib/python/site-packages/PIL/Image.py:2800: DecompressionBombWarning: Image size (141812970 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n"
     ]
    }
   ],
   "source": [
    "# ensure saved as JPG, from JPEG and PNG\n",
    "img = Path(IMG_DIR, \"original.jpeg\")\n",
    "save_as_jpg(img, IMG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](SingleImageProcess/original.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. rotate images to landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = Path(IMG_DIR, \"original.jpg\")\n",
    "actual_img = Image.open(str(image_path))\n",
    "width = actual_img.size[0]\n",
    "height = actual_img.size[1]\n",
    "\n",
    "# rotate to landscape if needed, don't yet know up or down\n",
    "if width < height:\n",
    "    rotated = actual_img.rotate(90, expand=True)\n",
    "rotated_name = str(Path(IMG_DIR, \"rotated.jpg\"))\n",
    "rotated.save(rotated_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](SingleImageProcess/rotated.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ensure pictures right-side up with template search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match(template, target):\n",
    "    method = cv2.TM_SQDIFF_NORMED  \n",
    "    result = cv2.matchTemplate(template, target, method) \n",
    "\n",
    "    # minimum squared difference\n",
    "    # image similarity score is maxVal\n",
    "    mn, maxVal, mnLoc, maxLoc = cv2.minMaxLoc(result)  \n",
    "\n",
    "    # exaggerate the values to make it easier to set a cutoff point\n",
    "    return (maxVal*100)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load images\n",
    "TEMPLATE = \"template.jpg\"\n",
    "template_img = cv2.imread(TEMPLATE, 1) #1 is grayscale enum flag\n",
    "\n",
    "img_path = Path(IMG_DIR, \"rotated.jpg\")\n",
    "test = str(img_path)\n",
    "test_img = cv2.imread(test, 1) #1 is grayscale enum flag\n",
    "\n",
    "# cropping box to reduce search area for target template\n",
    "left = 8500\n",
    "top = 8000\n",
    "right = left + 700\n",
    "bottom = top + 700\n",
    "\n",
    "# get the target image from the template\n",
    "# crop_img = img[y:y+h, x:x+w] #opencv's x and y are flipped\n",
    "cropped_img = test_img[top:bottom, left:right]\n",
    "score = round(find_match(template_img, cropped_img))\n",
    "\n",
    "# flip and save in place\n",
    "# 600 points seems to be a good cutoff\n",
    "if score < 600:\n",
    "    upright_image = cv2.rotate(test_img, cv2.ROTATE_180)\n",
    "    upright_name = str(Path(IMG_DIR, \"upright.jpg\"))\n",
    "\n",
    "cv2.imwrite(upright_name, upright_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](SingleImageProcess/upright.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. crop off margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cropping box to reduce search area for target template\n",
    "left = 850\n",
    "top = 710\n",
    "right = 12560\n",
    "bottom = 7300\n",
    "\n",
    "img_path = Path(IMG_DIR, \"upright.jpg\")\n",
    "file_name = str(img_path)\n",
    "img = cv2.imread(file_name, 1) #1 is grayscale enum flag        \n",
    "cropped_img = img[top:bottom, left:right]\n",
    "\n",
    "name = str(Path(IMG_DIR, \"cropped.jpg\"))\n",
    "cv2.imwrite(name, cropped_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](SingleImageProcess/cropped.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. downscale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cropped.jpg: (6590, 11710, 3), resized.jpg: (659, 1171, 3)\n"
     ]
    }
   ],
   "source": [
    "SCALE_PERCENT = 10 # percent of original size\n",
    "\n",
    "img_path = Path(IMG_DIR, \"cropped.jpg\")\n",
    "img_name = str(img_path)\n",
    "img = cv2.imread(img_name, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# calculate new size\n",
    "width = int(img.shape[1] * SCALE_PERCENT / 100)\n",
    "height = int(img.shape[0] * SCALE_PERCENT / 100)\n",
    "new_size = (width, height)\n",
    "\n",
    "# resize image\n",
    "resized = cv2.resize(img, new_size, interpolation = cv2.INTER_AREA)\n",
    "resized_name = str(Path(IMG_DIR, \"resized.jpg\"))\n",
    "cv2.imwrite(resized_name, resized)\n",
    "\n",
    "print(f\"cropped.jpg: {img.shape}, resized.jpg: {resized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](SingleImageProcess/resized.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. deskew images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load(img):\n",
    "    # load the image from disk\n",
    "    image = cv2.imread(str(img))\n",
    "\n",
    "    # convert to grayscale, flip foreground and background\n",
    "    # foreground is now \"white\" and the background is \"black\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.bitwise_not(gray)\n",
    "\n",
    "    # threshold the image, setting all foreground pixels to 255 and all background pixels to 0\n",
    "    return image, cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "def deskew(image, angle):\n",
    "    # rotate the image to deskew it\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    return cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "# load and get threshold\n",
    "img_path = Path(IMG_DIR, \"resized.jpg\")\n",
    "image, thresh = load(img_path)\n",
    "\n",
    "# grab the (x, y) coordinates of all pixel values that\n",
    "# are greater than zero, then use these coordinates to\n",
    "# compute a rotated bounding box that contains all\n",
    "# coordinates\n",
    "coords = np.column_stack(np.where(thresh > 0))\n",
    "angle = cv2.minAreaRect(coords)[-1]\n",
    "\n",
    "# the `cv2.minAreaRect` function returns values in the\n",
    "# range [-90, 0); as the rectangle rotates clockwise the\n",
    "# returned angle trends to 0 -- in this special case we\n",
    "# need to add 90 degrees to the angle\n",
    "if angle < -45:\n",
    "    angle = -(90 + angle)\n",
    "\n",
    "# otherwise, just take the inverse of the angle to make it positive\n",
    "else:\n",
    "    angle = -angle\n",
    "\n",
    "deskewd = deskew(image, angle)\n",
    "deskewd_name = str(Path(IMG_DIR, \"deskewd.jpg\"))\n",
    "cv2.imwrite(deskewd_name, deskewd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](SingleImageProcess/deskewd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. slice the images into 8 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CroppingTemplate:\n",
    "    def __init__(self):\n",
    "        self.image_file = None  # posix path\n",
    "        self.img = None  # actual image\n",
    "        self.counter = 1\n",
    "        self.dest_dir = \"./major_pieces\"\n",
    "        \n",
    "        self.left = 0\n",
    "        self.x1 = 330\n",
    "        self.x2 = 470\n",
    "        self.x3 = 990\n",
    "        self.right = None  # set in self.load()\n",
    "        \n",
    "        self.top = 0\n",
    "        self.y1 = 110\n",
    "        self.y2 = 235\n",
    "        self.y3 = 360\n",
    "        self.y4 = 510\n",
    "        self.bottom = None  # set in self.load()\n",
    "\n",
    "#     def load(self):\n",
    "#         self.img = Image.open(str(self.image_file))\n",
    "#         self.right = self.img.size[0]  # image width\n",
    "#         self.bottom = self.img.size[1]  # image height\n",
    "        \n",
    "#         # need right and bottom values before setting the section boxes\n",
    "#         self.setup_sections()\n",
    "\n",
    "#     def crop_all(self):\n",
    "#         for section in self.major_sections:\n",
    "#             for dir_name, box in section.items():\n",
    "#                 cropped_img = self.img.crop((box[\"left\"], box[\"top\"], box[\"right\"], box[\"bottom\"]))\n",
    "#                 file_name = path.join(self.dest_dir, dir_name, str(self.counter)+\".jpg\")\n",
    "#                 cropped_img.save(file_name)\n",
    "#         self.counter += 1\n",
    "        \n",
    "        \n",
    "    # This method was adapted from the original, just for this notebook\n",
    "    def single_image_process(self):\n",
    "        self.img = Image.open(str(self.image_file))\n",
    "        self.right = self.img.size[0]  # image width\n",
    "        self.bottom = self.img.size[1]  # image height\n",
    "        self.setup_sections()\n",
    "        \n",
    "        # crop out the parts based on the (manually set) template's dimensions\n",
    "        for section in self.major_sections:\n",
    "            for dir_name, box in section.items():\n",
    "                cropped_img = self.img.crop((box[\"left\"], box[\"top\"], box[\"right\"], box[\"bottom\"]))\n",
    "                file_name = path.join(\"SingleImageProcess\", dir_name+\".jpg\")\n",
    "#                 print(\"file name: \", file_name)\n",
    "                cropped_img.save(file_name)\n",
    "        \n",
    "    def setup_sections(self):\n",
    "        # these are the box boundaries for each section, manually set\n",
    "        self.major_sections = [\n",
    "            {\"uppercase\": {\n",
    "                \"left\": self.left,\n",
    "                \"top\": self.top,\n",
    "                \"right\": self.right,\n",
    "                \"bottom\": self.y1}\n",
    "            },\n",
    "            {\"lowercase\": {\n",
    "                \"left\": self.left,\n",
    "                \"top\": self.y1,\n",
    "                \"right\": self.right,\n",
    "                \"bottom\": self.y2}\n",
    "            },\n",
    "            {\"digits\": {\n",
    "                \"left\": self.left,\n",
    "                \"top\": self.y2,\n",
    "                \"right\": self.x2,\n",
    "                \"bottom\": self.y3}\n",
    "            },\n",
    "            {\"punctuation\": {\n",
    "                \"left\": self.x2,\n",
    "                \"top\": self.y2,\n",
    "                \"right\": self.right,\n",
    "                \"bottom\": self.y3}\n",
    "            },\n",
    "            {\"sent1\": {\n",
    "                \"left\": self.left,\n",
    "                \"top\": self.y3,\n",
    "                \"right\": self.x1,\n",
    "                \"bottom\": self.y4}\n",
    "            },\n",
    "            {\"sent2\": {\n",
    "                \"left\": self.x2,\n",
    "                \"top\": self.y3,\n",
    "                \"right\": self.x3,\n",
    "                \"bottom\": self.y4}\n",
    "            },\n",
    "            {\"sent3\": {\n",
    "                \"left\": self.left,\n",
    "                \"top\": self.y4,\n",
    "                \"right\": self.x1,\n",
    "                \"bottom\": self.bottom}\n",
    "            },\n",
    "            {\"sent4\": {\n",
    "                \"left\": self.x2,\n",
    "                \"top\": self.y4,\n",
    "                \"right\": self.x3,\n",
    "                \"bottom\": self.bottom}\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract major sections from each image\n",
    "img_path = Path(IMG_DIR, \"deskewd.jpg\")\n",
    "cropper = CroppingTemplate()\n",
    "cropper.image_file = img_path\n",
    "cropper.single_image_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uppercase\n",
    "![](SingleImageProcess/uppercase.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase\n",
    "![](SingleImageProcess/lowercase.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Digits\n",
    "![](SingleImageProcess/digits.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punctuation\n",
    "![](SingleImageProcess/punctuation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence 1\n",
    "![](SingleImageProcess/sent1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence 2\n",
    "![](SingleImageProcess/sent2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence 3\n",
    "![](SingleImageProcess/sent3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence 4\n",
    "![](SingleImageProcess/sent4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. extract individual boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = Path(IMG_DIR, \"uppercase.jpg\")\n",
    "file_name = str(img_path)\n",
    "\n",
    "# Read image\n",
    "uppercase_img = cv2.imread(file_name)\n",
    "gray = cv2.cvtColor(uppercase_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# bilateralFilter reduces noise while preserving boundaries\n",
    "gray = cv2.bilateralFilter(gray, 11, 17, 17)\n",
    "file_name = str(Path(IMG_DIR, \"bilateralFiltered.jpg\"))\n",
    "cv2.imwrite(file_name, gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bilateralFiltered.jpg\n",
    "![](SingleImageProcess/bilateralFiltered.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invert the image for making bounding boxes\n",
    "inverted = cv2.bitwise_not(gray)\n",
    "file_name = str(Path(IMG_DIR, \"bilateralFilteredInverted.jpg\"))\n",
    "cv2.imwrite(file_name, inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bilateralFilteredInverted.jpg\n",
    "![](SingleImageProcess/bilateralFilteredInverted.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get edges\n",
    "# edged = cv2.Canny(gray, 30, 200)\n",
    "edged = cv2.Canny(inverted, 30, 200)\n",
    "file_name = str(Path(IMG_DIR, \"canny.jpg\"))\n",
    "cv2.imwrite(file_name, edged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### canny.jpg\n",
    "![](SingleImageProcess/canny.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contours found: 10\n",
      "Length of first contour: 190\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# find contours in the edged image, keep only the largest ones, and initialize our screen contour\n",
    "contours = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours = imutils.grab_contours(contours) # convenience function (really simple, see https://github.com/jrosebr1/imutils/blob/master/imutils/convenience.py)\n",
    "contours = sorted(contours, key = cv2.contourArea, reverse=True)[:10]\n",
    "print(f\"Contours found: {len(contours)}\")\n",
    "print(f\"Length of first contour: {len(contours[0])}\")\n",
    "print(type(contours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenCnt = None\n",
    "# loop over our contours\n",
    "for c in contours:\n",
    "    # approximate the contour\n",
    "    epsilon = 0.015 * cv2.arcLength(c, True)\n",
    "    approx = cv2.approxPolyDP(c, epsilon, True)\n",
    "    \n",
    "    # if our approximated contour has four points, then we can assume that we have found our screen\n",
    "    if len(approx) == 4:\n",
    "        screenCnt = approx\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opencv colors\n",
    "blue = (255, 0, 0)\n",
    "red = (0, 0, 255)\n",
    "green = (0, 255, 0)\n",
    "black = (0, 0, 0)\n",
    "purple = (255, 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv2.drawContours(uppercase_img, screenCnt, -1, (0, 255, 0), 3)\n",
    "boxed_image = cv2.drawContours(uppercase_img, contours, -1, green, 1)\n",
    "boxed_name = str(Path(IMG_DIR, \"boxed.jpg\"))\n",
    "cv2.imwrite(boxed_name, boxed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](SingleImageProcess/boxed.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with getting individual boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring how to isolate the row of handwritten letters.\n",
    "# get the minmax of x/y values\n",
    "# save the first contoured image\n",
    "# first = contours[0]\n",
    "# first.shape   # (190, 1, 2)\n",
    "# first[0]   # array([[233,   6]], dtype=int32)\n",
    "# first is an array of [[x, y]] points\n",
    "# first[0][0][0]   # 233\n",
    "# first[0][0][1]   # 6\n",
    "\n",
    "coord = lambda point: tuple(point[0][0])  # tuple(x, y)\n",
    "radius = 10\n",
    "thickness = -1 # negative for filled circle\n",
    "\n",
    "# draw a point to see where it is in the image\n",
    "# image = cv.circle(image, centerOfCircle, radius, color, thickness)\n",
    "dotted = cv2.circle(boxed_image, coord(contours[0]), radius, blue, thickness)\n",
    "file_name = str(Path(IMG_DIR, \"dotted.jpg\"))\n",
    "cv2.imwrite(file_name, dotted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](SingleImageProcess/dotted.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xmin 10  Xmax 892  Ymin 6  Ymax 51\n"
     ]
    }
   ],
   "source": [
    "# get minmax of x and y values\n",
    "x = lambda point: point[0][0][0]\n",
    "y = lambda point: point[0][0][1]\n",
    "\n",
    "xs = [x(contour) for contour in contours]\n",
    "ys = [y(contour) for contour in contours]\n",
    "\n",
    "x_min = min(xs)\n",
    "x_max = max(xs)\n",
    "y_min = min(ys)\n",
    "y_max = max(ys)\n",
    "print(f\"Xmin {x_min}  Xmax {x_max}  Ymin {y_min}  Ymax {y_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the four extremes of the first contour\n",
    "upper_left = (x_min, y_min)\n",
    "lower_left = (x_min, y_max)\n",
    "upper_right = (x_max, y_min)\n",
    "lower_right = (x_max, y_max)\n",
    "\n",
    "four_corners = cv2.circle(uppercase_img, upper_left, radius, red, thickness)\n",
    "four_corners = cv2.circle(uppercase_img, upper_right, radius, green, thickness)\n",
    "four_corners = cv2.circle(uppercase_img, lower_left, radius, purple, thickness)\n",
    "four_corners = cv2.circle(uppercase_img, lower_right, radius, black, thickness)\n",
    "file_name = str(Path(IMG_DIR, \"contour1.jpg\"))\n",
    "cv2.imwrite(file_name, four_corners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](SingleImageProcess/contour1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'area': 88150, 'corners': (133, 1158, 6, 92)},\n",
      " 1: {'area': 21746, 'corners': (275, 537, 6, 89)},\n",
      " 2: {'area': 6825, 'corners': (892, 1067, 50, 89)},\n",
      " 3: {'area': 7000, 'corners': (892, 1067, 8, 48)},\n",
      " 4: {'area': 5070, 'corners': (539, 669, 50, 89)},\n",
      " 5: {'area': 3731, 'corners': (759, 850, 50, 91)},\n",
      " 6: {'area': 11039, 'corners': (7, 140, 9, 92)},\n",
      " 7: {'area': 2604, 'corners': (363, 425, 6, 48)},\n",
      " 8: {'area': 1932, 'corners': (407, 453, 6, 48)},\n",
      " 9: {'area': 1720, 'corners': (9, 52, 50, 90)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wandalcooper/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:15: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "def four_corners(box):\n",
    "    xs = [el[0][0] for el in box]\n",
    "    ys = [el[0][1] for el in box]\n",
    "    x_min = min(xs)\n",
    "    x_max = max(xs)\n",
    "    y_min = min(ys)\n",
    "    y_max = max(ys)\n",
    "    return (x_min, x_max, y_min, y_max)\n",
    "\n",
    "def area(corners):\n",
    "    return (corners[1] - corners[0]) * (corners[3] - corners[2])\n",
    "\n",
    "boxes = {}\n",
    "for contour in contours:\n",
    "    box = contours.index(contour)\n",
    "    corners = four_corners(contour)\n",
    "    boxes[box] = {\"area\": area(corners), \"corners\": corners}\n",
    "\n",
    "pprint(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'area': 88150, 'corners': (133, 1158, 6, 92)}\n"
     ]
    }
   ],
   "source": [
    "# box with the greatest area\n",
    "biggest_area = 0\n",
    "biggest_box = None\n",
    "for box, values in boxes.items():\n",
    "    if values[\"area\"] > biggest_area:\n",
    "        biggest_area = values[\"area\"]\n",
    "        biggest_box = values\n",
    "print(biggest_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crop the image again using the extremes of the largest-area box\n",
    "corners = biggest_box[\"corners\"]\n",
    "left = corners[0]\n",
    "top = corners[2]\n",
    "right = corners[1]\n",
    "bottom = corners[3]\n",
    "\n",
    "# crop_img = img[y:y+h, x:x+w] #opencv's x and y are flipped\n",
    "canny_cropped = uppercase_img[top:bottom, left:right]\n",
    "file_name = str(Path(IMG_DIR, \"croppedAfterCanny.jpg\"))\n",
    "cv2.imwrite(file_name, canny_cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going by just the biggest area doesn't work as there may be cut off as on the left...\n",
    "\n",
    "![](SingleImageProcess/croppedAfterCanny.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 1158 6 92\n"
     ]
    }
   ],
   "source": [
    "# the minmax across all the box corners\n",
    "# for box, values in boxes.items()\n",
    "xmin = min([values[\"corners\"][0] for box, values in boxes.items()])\n",
    "xmax = max([values[\"corners\"][1] for box, values in boxes.items()])\n",
    "ymin = min([values[\"corners\"][2] for box, values in boxes.items()])\n",
    "ymax = max([values[\"corners\"][3] for box, values in boxes.items()])\n",
    "\n",
    "print(xmin, xmax, ymin, ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the original image\n",
    "img_path = Path(IMG_DIR, \"uppercase.jpg\")\n",
    "file_name = str(img_path)\n",
    "\n",
    "# Read image\n",
    "uppercase_img = cv2.imread(file_name)\n",
    "\n",
    "# crop the image again using the extremes of all the contours\n",
    "left = xmin\n",
    "top = ymin\n",
    "right = xmax\n",
    "bottom = ymax\n",
    "\n",
    "# get the target image from the template\n",
    "# crop_img = img[y:y+h, x:x+w] #opencv's x and y are flipped\n",
    "canny_cropped_all_extremes = uppercase_img[top:bottom, left:right]\n",
    "file_name = str(Path(IMG_DIR, \"croppedAfterCanny-allextremes.jpg\"))\n",
    "cv2.imwrite(file_name, canny_cropped_all_extremes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This worked well. Getting the extreme edges using Canny and cropping the image as close as possbile to the boxes\n",
    "\n",
    "![](SingleImageProcess/croppedAfterCanny-allextremes.jpg)  \n",
    "\n",
    "Because this is a template image, perhaps just dividing the boxes up can be done much simpler than using opencv directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width = canny_cropped_all_extremes.shape[1]\n",
    "height = canny_cropped_all_extremes.shape[0]\n",
    "\n",
    "border = 1.5\n",
    "boxes_wide = 26\n",
    "rows_high = 2\n",
    "l_top = round((height/2) + 2* border) # compensate for border\n",
    "l_bottom = round(height - border)\n",
    "#L is between 11th and 12th vertical line\n",
    "l_left = round((width/boxes_wide) * 11 + (11*border))\n",
    "l_right = round(width/boxes_wide) * 12\n",
    "\n",
    "# crop out the letter L to test\n",
    "left = l_left # xmin\n",
    "top = l_top # ymin\n",
    "right = l_right # xmax\n",
    "bottom = l_bottom # ymax\n",
    "\n",
    "# a single letter\n",
    "letter_l = uppercase_img[top:bottom, left:right]\n",
    "file_name = str(Path(IMG_DIR, \"letter_l.jpg\"))\n",
    "cv2.imwrite(file_name, letter_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a single letter extracted from the uppercase template...  \n",
    "![](SingleImageProcess/letter_l.jpg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
